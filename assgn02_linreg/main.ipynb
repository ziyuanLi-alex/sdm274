{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') # 在Ubuntu上执行代码，需要指定恰当的绘图后端\n",
    "\n",
    "# 生成数据，使用线性模型 y = w * x + b + noise，指定seed，保证每次运行代码生成的数据一致\n",
    "np.random.seed(42)\n",
    "X_train = np.arange(100).reshape(100,1)\n",
    "w, b  = 1, 10\n",
    "y_train = w * X_train + b + np.random.normal(0,5,size=X_train.shape)\n",
    "y_train = y_train.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面定义了数据，下面来定义类和相关的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, epochs = 1000, lr = 0.00001, X = None, y = None):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.SGD_losses = np.zeros(epochs)\n",
    "        self.BGD_losses = np.zeros(epochs)\n",
    "        self.MBGD_losses = np.zeros(epochs)\n",
    "\n",
    "    def _loss(self, y, y_pred):\n",
    "        return np.sum((y_pred - y) ** 2) / y.size\n",
    "\n",
    "    def _gradient(self, X, y, y_pred):\n",
    "        return (y_pred - y) @ X / y.size\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b \n",
    "\n",
    "    def fit_SGD(self, X, y, plot=False, normalize=None):\n",
    "\n",
    "        # Apply normalization if specified\n",
    "        if normalize == 'min-max':\n",
    "            X = self.min_max_normalize(X)\n",
    "        elif normalize == 'mean':\n",
    "            X = self.mean_normalize(X)\n",
    "\n",
    "        n_samples, n_features = X.shape \n",
    "        self.w = np.random.rand(X.shape[1]) \n",
    "        self.b = np.random.rand(1)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            shuffle_index = np.random.permutation(n_samples)\n",
    "            X = X[shuffle_index]\n",
    "            y = y[shuffle_index]\n",
    "            epoch_loss = self._loss(y, self.predict(X))\n",
    "            self.SGD_losses[epoch] = epoch_loss\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                y_pred = self.predict(X[i])\n",
    "                loss = self._loss(y[i], y_pred)\n",
    "                grad = self._gradient(X[i], y[i], y_pred)\n",
    "\n",
    "                self.w -= self.lr * grad\n",
    "                self.b -= self.lr * grad\n",
    "            \n",
    "        if plot:\n",
    "            plt.plot(self.SGD_losses)\n",
    "            plt.show()\n",
    "\n",
    "    def fit_BGD(self, X, y, plot=False, normalize=None):\n",
    "        \n",
    "        # Apply normalization if specified\n",
    "        if normalize == 'min-max':\n",
    "            X = self.min_max_normalize(X)\n",
    "        elif normalize == 'mean':\n",
    "            X = self.mean_normalize(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.random.rand(X.shape[1]) \n",
    "        self.b = np.random.rand(1)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = self._loss(y, self.predict(X))\n",
    "            self.BGD_losses[epoch] = epoch_loss \n",
    "\n",
    "            y_pred = self.predict(X)\n",
    "            loss = self._loss(y, y_pred)\n",
    "            grad = self._gradient(X, y, y_pred)\n",
    "            self.w -= self.lr * grad\n",
    "            self.b -= self.lr * grad\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(self.BGD_losses)\n",
    "            plt.show()\n",
    "\n",
    "    def fit_MBGD(self, X, y, batch_size=10, plot=False, normalize=None):\n",
    "\n",
    "        # Apply normalization if specified\n",
    "        if normalize == 'min-max':\n",
    "            X = self.min_max_normalize(X)\n",
    "        elif normalize == 'mean':\n",
    "            X = self.mean_normalize(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.random.rand(X.shape[1])\n",
    "        self.b = np.random.rand(1)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            shuffle_index = np.random.permutation(n_samples)\n",
    "            X = X[shuffle_index]\n",
    "            y = y[shuffle_index]\n",
    "\n",
    "            epoch_loss = self._loss(y, self.predict(X))\n",
    "            self.MBGD_losses[epoch] = epoch_loss\n",
    "\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "\n",
    "                y_pred = self.predict(X_batch)\n",
    "                loss = self._loss(y_batch, y_pred)\n",
    "\n",
    "                grad = self._gradient(X_batch, y_batch, y_pred)\n",
    "                self.w -= self.lr * grad\n",
    "                self.b -= self.lr * grad\n",
    "        \n",
    "        if plot:\n",
    "            plt.plot(self.MBGD_losses)\n",
    "            plt.show()\n",
    "\n",
    "    def plot_Xy(self, X, y, title=None):\n",
    "        plt.scatter(X, y, label='Data')\n",
    "        plt.plot(X, w * X + b, color='red', label='Ideal')\n",
    "        plt.plot(X, self.predict(X), color='green', label='Prediction')\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.BGD_losses, label='BGD', color='red')\n",
    "        plt.plot(self.SGD_losses, label='SGD', color='green')\n",
    "        plt.plot(self.MBGD_losses, label='MBGD', color='blue')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def min_max_normalize(self, X):\n",
    "        X_min = np.min(X, axis=0)\n",
    "        X_max = np.max(X, axis=0)\n",
    "        return (X - X_min) / (X_max - X_min)\n",
    "\n",
    "    def mean_normalize(self, X):\n",
    "        X_mean = np.mean(X, axis=0)\n",
    "        X_min = np.min(X, axis=0)\n",
    "        X_max = np.max(X, axis=0)\n",
    "        return (X - X_mean) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.00001, 0.000001]\n",
    "\n",
    "# Create a figure to plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterate over the learning rates\n",
    "for lr in learning_rates:\n",
    "    # Initialize the LinearRegression model with the current learning rate\n",
    "    model = LinearRegression(epochs=200, lr=lr, X=X_train, y=y_train)\n",
    "    \n",
    "    # Fit the model using SGD\n",
    "    model.fit_SGD(X_train, y_train)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.plot(model.SGD_losses, label=f'LR={lr}')\n",
    "\n",
    "# Add labels and legend to the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SGD Loss Curves for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.savefig('sgd_loss_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当学习率过大时，容易算出非常大的loss function，以至于会越界：\n",
    "\n",
    "\n",
    "```python\n",
    "/tmp/ipykernel_53127/1945371640.py:13: RuntimeWarning: overflow encountered in square\n",
    "  return np.sum((y_pred - y) **2 ) / y.size\n",
    "/tmp/ipykernel_53127/1945371640.py:16: RuntimeWarning: overflow encountered in matmul\n",
    "  return (y_pred - y) @ X / y.size\n",
    "/tmp/ipykernel_53127/1945371640.py:47: RuntimeWarning: invalid value encountered in subtract\n",
    "  self.w -= self.lr * grad\n",
    "/tmp/ipykernel_53127/1945371640.py:48: RuntimeWarning: invalid value encountered in subtract\n",
    "  self.b -= self.lr * grad\n",
    "```\n",
    "\n",
    "这个较大的学习率在0.0005左右。\n",
    "\n",
    "\n",
    "<img src=\"image.png\"  width=\"500px\">\n",
    "\n",
    "可以看到，在采用较小的学习率后，模型收敛的速度更慢了，但是同时也更加稳定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140455773823680process_stream_events\"\n",
      "    while executing\n",
      "\"140455773823680process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "epoch_numbers = [10, 50, 100]\n",
    "\n",
    "# Create a figure to plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterate over the epoch numbers\n",
    "for epochs in epoch_numbers:\n",
    "    # Initialize the LinearRegression model with the current number of epochs\n",
    "    model = LinearRegression(epochs=epochs, lr=lr, X=X_train, y=y_train)\n",
    "    \n",
    "    # Fit the model using SGD\n",
    "    model.fit_SGD(X_train, y_train)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.plot(model.SGD_losses, label=f'Epochs={epochs}')\n",
    "\n",
    "# Add labels and legend to the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SGD Loss Curves for Different Epoch Numbers')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('sgd_loss_curves_epochs.png')\n",
    "plt.show()\n",
    "\n",
    "# Show predicted result and the scatterplot\n",
    "# model.plot_Xy(X_train, y_train, title='SGD Prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这一个线性回归的实验，收敛的次数不多。较多的epoch数对实验结果没有非常大的影响，典型的收敛epoch在50以下。\n",
    "这里采用了对数的y轴坐标来让结果更加直观。\n",
    "（图片粘贴好像出bug了）\n",
    "如果我们将预测出的曲线和原曲线进行比较，可以看到预测出的曲线和原曲线的拟合程度还是比较高的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140455752083072process_stream_events\"\n",
      "    while executing\n",
      "\"140455752083072process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "# Show predicted result and the scatterplot\n",
    "model.plot_Xy(X_train, y_train, title='SGD Prediction')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140455753821760process_stream_events\"\n",
      "    while executing\n",
      "\"140455753821760process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LinearRegression model\n",
    "model = LinearRegression(epochs=100, lr=0.000001, X=X_train, y=y_train)\n",
    "\n",
    "# Fit the model using SGD\n",
    "model.fit_SGD(X_train, y_train)\n",
    "sgd_losses = model.SGD_losses\n",
    "\n",
    "# Fit the model using BGD\n",
    "model.fit_BGD(X_train, y_train)\n",
    "bgd_losses = model.BGD_losses\n",
    "\n",
    "# Fit the model using MBGD\n",
    "model.fit_MBGD(X_train, y_train, batch_size=10)\n",
    "mbgd_losses = model.MBGD_losses\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(sgd_losses, label='SGD', color='green')\n",
    "plt.plot(bgd_losses, label='BGD', color='red')\n",
    "plt.plot(mbgd_losses, label='MBGD', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss Curves for SGD, BGD, and MBGD')\n",
    "plt.legend()\n",
    "plt.savefig('comparison_loss_curves.png')\n",
    "plt.show()\n",
    "\n",
    "model.plot_Xy(X_train, y_train, title='SGD Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是几种梯度下降方法的比较。可以发现：\n",
    "绿色曲线SGD的下降速度最快，红色曲线BGD的下降速度比较平缓，蓝色曲线的下降速度位于两者之间。\n",
    "\n",
    "SGD在每次更新权重的时候都使用单个样本，一个epoch会对参数进行samples数量的更新，所以在训练的初期下降速度非常快。但是，它可能会造成波动，这在之后的实验中可以看出。SGD是一种比较“高效”的梯度下降方式，但是可能不那么精准。\n",
    "\n",
    "BGD则处于SGD的反面，每次迭代都取整个数据集的梯度。由于BGD每次使用整个数据集更新，因此曲线非常平滑，没有SGD和MBGD那样的波动。它需要使用更多的存储资源。\n",
    "\n",
    "MBGD在训练初期损失值下降较快，但不及SGD，速度介于SGD和BGD之间。这是因为MBGD每次使用一小批数据来更新权重，批次较小导致更新频率比BGD高，但比SGD要慢。\n",
    "\n",
    "\n",
    "几种方法的初值不太一样，可以理解为随机化参数造成的误差。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LinearRegression model for min-max normalization\n",
    "model_min_max = LinearRegression(epochs=2500, lr=0.00005, X=X_train, y=y_train)\n",
    "\n",
    "# Fit the model using SGD with min-max normalization\n",
    "model_min_max.fit_SGD(X_train, y_train, normalize='min-max')\n",
    "sgd_losses_min_max = model_min_max.SGD_losses\n",
    "\n",
    "# Initialize the LinearRegression model for mean normalization\n",
    "model_mean = LinearRegression(epochs=2500, lr=0.00005, X=X_train, y=y_train)\n",
    "\n",
    "# Fit the model using SGD with mean normalization\n",
    "model_mean.fit_SGD(X_train, y_train, normalize='mean')\n",
    "sgd_losses_mean = model_mean.SGD_losses\n",
    "\n",
    "# Plot the loss curves for both normalization methods\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(sgd_losses_min_max, label='SGD with Min-Max Normalization', color='blue')\n",
    "plt.plot(sgd_losses_mean, label='SGD with Mean Normalization', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('SGD Loss Curves for Different Normalization Methods')\n",
    "plt.legend()\n",
    "plt.savefig('sgd_loss_curves_normalization.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = model_mean.mean_normalize(X_train)\n",
    "plt.plot(X_train_mean, model_mean.predict(X_train_mean), label='Mean Normalization')\n",
    "plt.scatter(X_train_mean, y_train, label='Data')\n",
    "plt.show()\n",
    "\n",
    "X_train_min_max = model_min_max.min_max_normalize(X_train)\n",
    "plt.plot(X_train_min_max, model_min_max.predict(X_train_min_max), label='Min-Max Normalization')\n",
    "plt.scatter(X_train_min_max, y_train, label='Data')\n",
    "plt.show()\n",
    "\n",
    "# model_mean.plot_Xy(X_train, model_mean.predict(X_train), title='SGD Prediction with Mean Normalization')\n",
    "# model_min_max.plot_Xy(X_train, model_min_max.predict(X_train), title='SGD Prediction with Min-Max Normalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
